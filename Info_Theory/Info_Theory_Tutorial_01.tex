\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

%\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
%\lhead{MA4413 2013} \rhead{Mr. Kevin O'Brien}
%\chead{Midterm Assessment 1 }
%\input{tcilatex}

\begin{document}


%---------------------------------------------------------------- %
\newpage
\noindent {\Large \textbf{MA4413 Weeks 12 and 13 Tutorials}}

%------------------------------------------------------------ %
\subsection*{Question 1a}
Consider a source $X$ that produces five symbols with probabilities 1/2, 1/4, 1/8, 1/16 and 1/16. Determine the source entropy $H(x)$. 

\subsection*{Question 1b}

A input source is a random variable X with a four letter alphabet $\{A,B,C,D\}$.
There are four different probability distributions presented below. Compute the entropy for each case.
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline	&	$X_i$	&	A	&	B	&	C	&	D	\\ \hline
Case 1	&	$p(X_i)$	&	0.25	&	0.25	&	0.25	&	0.25	\\ \hline
Case 2	&	$p(X_i)$	&	0.25	&	0.5	&	0.125	&	0.125	\\ \hline
Case 3	&	$p(X_i)$	&	0.7	&	0.1	&	0.1	&	0.1	\\ \hline
Case 4	&	$p(X_i)$	&	0.97	&	0.01	&	0.01	&	0.01	\\ \hline
\end{tabular} 
\end{center}

\noindent {\Large \textbf{MA4413 Weeks 12 and 13 Tutorials Solutions}}


\section*{Question 1b Solutions}

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline	&	Xi	&	A 	&	B	&	C	&	D	&	Entropy	\\ \hline \hline
Case 1	&	$p(x_i)$	&	0.2500	&	0.2500	&	0.2500	&	0.2500	&		\\ \hline
	&	$I(x_i)$	&	2.0000	&	2.0000	&	2.0000	&	2.0000	&		\\ \hline
	&	$p(x_i)\times I(x_i)$	&	0.5000	&	0.5000	&	0.5000	&	0.5000	&	2.0000	\\ \hline \hline
Case 2	&	$p(x_i)$	&	0.2500	&	0.1250	&	0.1250	&	0.1250	&		\\ \hline
	&	$I(x_i)$	&	2.0000	&	3.0000	&	3.0000	&	3.0000	&		\\ \hline
	&	$p(x_i)\times I(x_i)$	&	0.5000	&	0.3750	&	0.3750	&	0.3750	&	1.6250	\\ \hline \hline
Case 3	&	$p(x_i)$	&	0.7000	&	0.1000	&	0.1000	&	0.1000	&		\\ \hline
	&	$I(x_i)$	&	0.5146	&	3.3219	&	3.3219	&	3.3219	&		\\ \hline
	&	$p(x_i)\times I(x_i)$	&	0.3602	&	0.3322	&	0.3322	&	0.3322	&	1.3568	\\ \hline \hline
Case 4 	&	$p(x_i)$	&	0.9700	&	0.0100	&	0.0100	&	0.0100	&		\\ \hline
	&	$I(x_i)$	&	0.0439	&	6.6439	&	6.6439	&	6.6439	&		\\ \hline
	&	$p(x_i)\times I(x_i)$	&	0.0426	&	0.0664	&	0.0664	&	0.0664	&	0.2419	\\ \hline 

\end{tabular} 
\end{center}
\section*{Question 1c Solutions}

For source alphabet of size $m$ where each symbol has an equal probability of occurring
\[H(X) = \mbox{log}_2(m)\]
Here there are 8 equally probable symbols.
\[H(X) = \mbox{log}_2(8) = 3\mbox{b/sym}\]


\subsection*{Question 1c}
Consider a source $X$ that produces 8 symbols with equal probabilities for each symbol. Determine the source entropy $H(x)$. 
%------------------------------------------------------------ %


\subsection*{Question 2}

The input source to a noisy communication channel is a random variable X over three symbols $\{a,b,c\}$. The output from this channel is a random variable $Y$ over the same three symbols. The joint distribution of the these two random variables is as follows:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
	&	x=a	&	x=b	&	x=c	\\ \hline
y=a	&	0.25	&	0	&	0.125	\\ \hline
y=b	&	0	&	0.125	&	0	\\ \hline
y=c	&	0.125	&	0.25	&	0.125	\\ \hline
\end{tabular} 
\end{center}

\begin{itemize}
\item Write down the marginal distributions for X and Y.

\item Compute the marginal entropies $H(X)$ and $H(Y)$

\item Compute the joint entropy $H(X,Y)$ of the two random variables.
\end{itemize}
\newpage
\section*{Question 2 Solutions}
Marginal probabilities
\begin{center}
\begin{tabular}{|c|c|c|c||c|}
\hline	&	y=a	&	y=b	&	y=c	&		\\ \hline
x=a	&	0.25	&		&	0.125	&	\textbf{0.375}	\\ \hline
x=b	&		&	0.125	&	0	&	\textbf{0.125}	\\ \hline
x=c	&	0.125	&	0.25	&	0.125	&	\textbf{0.5	}\\ \hline \hline
	&	\textbf{0.375}	&	\textbf{0.375}	&	\textbf{0.25}	&	1	\\ \hline
\end{tabular}
\end{center}
Marginal Entropies : $H(X)$ and $H(Y)$
%--------------------------------------------- %
\begin{center}
\begin{tabular}{|c|c|c||c||c|c|c|} \hline
$P(x_i)$	&	$I(x_i)$	&	$P(x_i) \times I(x_i)$	&&	$P(y_i)$	&	$I(y_i)$	&	$P(y_i) \times I(y_i)$	\\ \hline \hline
0.3750	&	1.4150	&	0.5306	&&	0.3750	&	1.4150	&	0.5306	\\ \hline
0.1250	&	3.0000	&	0.3750	&&	0.3750	&	1.4150	&	0.5306	\\ \hline
0.5000	&	1.0000	&	0.5000	&&	0.2500	&	2.0000	&	0.5000	\\ \hline \hline
	&	$H(X)$	&	1.4056 b/sym	&&		&	$H(Y)$	&	1.5613 b/sym	\\ \hline
\end{tabular} 
\end{center}
Joint Entropy : $H(X,Y)$
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
$P(x_i)$	&	Freq($x_i$)	&	$I(x_i)$	&	$p(x_i)\times I(x_i) \times \mbox{Freq}(x_i)$	\\ \hline
0	&	3	&	\ldots &	0	\\ \hline
0.125	&	4	&	3	&	1.5	\\ \hline
0.25	&	2	&	2	&	1	\\ \hline \hline
& &H(X,Y) & 2.5 \\ \hline
\end{tabular} 
\end{center}



\newpage 
\subsection*{Question 3a}
A four letter alphabet is encoded into binary form according to
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline Case 1	&	A:  10 	&	C:  110	&	G:  111 &	T:  0  \\ \hline
Case 2	&	A:  00	&	C:  01	&		G: 10	&	T: 11	\\ \hline
\end{tabular} 
\end{center}
Using the code presented in case 1, decode the following sequence:
\begin{verbatim}	
11110001011010
\end{verbatim}

\noindent Encode this message using the code from case 2. Compare the length of messages in both cases.

\subsection*{Question 3b}
Given that the alphabet has the following distribution 
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x_i$	& A	& C	& G	& T \\ \hline
$p(x_i)$	& 0.25	& 0.125	& 0.125	& 0. 5 \\ \hline
\end{tabular} 
\end{center}
Compute the average symbol length for both cases.



%----------------------------------------------------------------------------- %
\section*{Question 3b}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
	&	A	&	C	&	G	&	T	&		\\ \hline	
Code 1	&	10	&	110	&	111	&	0	&		\\ \hline	
$p(x_i)$	&	0.25	&	0.125	&	0.125	&	0.5	&		\\ \hline	
$n(x_i)$	&	2	&	3	&	3	&	1	&		\\ \hline	
$p(x_i) \times n(x_i)$	&	0.5	&	0.375	&	0.375	&	0.5	&	\textbf{1.75}	\\ \hline	
	&		&		&		&		&		\\ \hline	\hline
Code 2	&	00	&	01	&	10	&	11	&		\\ \hline	
$p(x_i)$	&	0.25	&	0.125	&	0.125	&	0.5	&		\\ \hline	
$n(x_i)$	&	2	&	2	&	2	&	2	&		\\ \hline	
$p(x_i) \times n(x_i)$	&	0.5	&	0.25	&	0.25	&	1	&	\textbf{2}	\\ \hline	
\end{tabular} 
\end{center}

Expected Code Lengths $E(L)$ for Codes 1 and 2 are 1.75b and 2b respectively.

%----------------------------------------------------------------------------- %
\section*{Question 5}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline	&		&	Shannon Fano	&	Huffman	&		&		&		&	\\	
	&		&	 Code	&	Code	&	$n(x_i)$	&	$p(x_i) \times n(x_i)$	&	$I(x_i)$	&	$p(x_i) \times I(x_i)$	\\ \hline
$x_5$	&	0.50	&		&		&	1	&	0.50	&	1	&	0.5000	\\ \hline
$x_1$	&	0.20	&		&		&	2	&	0.40	&	2.3219	&	0.4644	\\ \hline
$x_2$	&	0.15	&		&		&	3	&	0.45	&	2.7370	&	0.4105	\\ \hline
$x_4$	&	0.10	&		&		&	4	&	0.40	&	3.3219	&	0.3322	\\ \hline
$x_3$	&	0.05	&		&		&	4	&	0.20	&	4.3219	&	0.2161	\\ \hline
	&		&		&		&		&	1.95	&		&	1.9232	\\ \hline
 
\end{tabular} 
\end{center}

\newpage 
%-----------------------------------------%
\subsection*{Question 4}
A DMS X has live symbols $\{x_l,x_2,x_3,x_4,x_5\}$ with $P(x_1) = 0.4$, $P(x_2)=0.19$, $P(x_3) =0.16$,
$P(x_4) = 0.15$, and $P(x_5) = 0.1$.
\begin{itemize}
\item[(a)] Construct the Shannon-Fano code for X, and calculate the efficiency of the code.
\item[(b)] Repeat for the Huffman code and compare the results.
\end{itemize}

\subsection*{Question 5}

A discrete memoryless source has a five symbol alphabet $\{x_1,x_2,x_3,x_4,x_5\}$ with the following probabilities 0.2,0.15,0.05,0.10 and 0.5.

\begin{itemize}
\item[(i)] Construct a Shannon-Fano code for $X$, and calculate the code efficiency.
\item[(ii)] Construct a Huffman code for $X$, and calculate the code efficiency.
\end{itemize}

%------------------------------------------------------------ %

%------------------------------------------------------------ %
\subsection*{Question 6}

The input source to a noisy communication channel is a random variable X over three symbols a,b,c. The output from this channel is a random variable Y over the same three symbols. The joint distribution of these two random variables is as follows:


\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline	 &x=a	&x=b&	     x=c\\ \hline
y=a	 &0.25	&0	 &    0.125 \\ \hline
y=b	 &0	    &0.125&	 0 \\ \hline
y=c	 &0.125&	0.25&	 0.125 \\ \hline
\end{tabular} 
\end{center}

\begin{itemize}
\item Write down the marginal distributions for X and Y. %(Last Week)
\item Compute the marginal entropies $H(X)$ and $H(Y)$ %(Last Week)
\item Compute the joint entropy $H(X,Y)$ of the two random variables. % (Last Week)
\item
Compute the mutual information $I(X;Y)$.
\item 
Compute the conditional entropies $H(X|Y)$ and $H(Y|X)$.
\item 
From Formulae: \[I(X,Y) = H(X) — H(X|Y)\]
\end{itemize}
%------------------------------------------------------------ %
\subsection*{Question 7}

The frequency of 0 as an input to a binary channel is 0.6. If 0 is the input, then 0 is the output with probability 0.8. If l is the input, then l is the output with probability 0.9.

Write out the channel transition matrix

\begin{itemize}
\item	Calculate the output probabilities [P(Y)] 
\item	Compute the joint probabilities [P(X,Y)]
\item	Calculate the probability that the input is 0 given that the output is O. 
\item Calculate the probability that the input is l given that the output is 1, 
\item	Calculate the probability that the input is l given that the output is O.
\item	Calculate the probability that the input is 0 given that the output is l. 
\end{itemize}
%------------------------------------------------------------ %

%------------------------------------------------------------ %
%\section*{Question 4}

%-----------------------------------------%
\subsection*{Question 8 }
Consider a DMS X with symbols $\{x_l,x_2,x_3,x_4\}$. The table below lists four possible
binary codes.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{./12ACodes}
\caption{}
\label{fig:12ACodes}
\end{figure}



%IMAGE OF KRAFT INEQUALITY HERE
\begin{itemize}
\item[(i)] Show that all codes except B satisfy the Kraft inequality (formula below)
m
\item[(ii)] Show that codesA and D are uniquely decodable but code B and C are not
uniquely demdable.
\end{itemize}
\begin{figure}[h!]
\centering
\includegraphics[width=0.4\linewidth]{./12AKraftIneq}
\caption{}
\label{fig:12AKraftIneq}
\end{figure}


%-----------------------------------------%
\subsection*{Question 9}
A DMS X has live symbols $\{x_l,x_2,x_3,x_4,x_5\}$ with $P(x_1) = 0.2$, $P(x_2)=0.15$, $P(x_3) =0.05$,
$P(x_4) = 0.10$, and $P(x_5) = 0.5$. 
\begin{itemize}
\item[(a)] Construct a Shann0n—Fano code for X, and calculate the efficiency of the code.
\item[(b)] Repeat for the Huffman code and compare the results.
\end{itemize}

 
\end{document}
\item
