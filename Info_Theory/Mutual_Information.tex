

\noindent \textbf{Mutual Information}
Mutual information is one of many quantities that measures how much one random variables gives about another. It is a dimensionless quantity. Mutual Informaiton can be thought of as the reduction in uncertainty about one random variable given knowledge of another. 
\begin{itemize}\item High mutual information indicates a large reduction in uncertainty, \item low mutual information indicates a small reduction, \item  zero mutual information between two random variables means the variables are independent.
\end{itemize}
Efficient communication systems have high mutual information.

%---------------------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information}
\vspace{-1cm}
\textbf{Joint Entropies:}\\
Using the input probabilities $P(x_i)$, output probabilities $P(y_i)$, transition probabilities $P(y_i|x_i)$,
and joint probabilities $P(x_i,y_j)$, we can define the following various entropy functions for a channel
with m inputs and n outputs:

\begin{itemize}
\item $H(X) = - \sum ^{m}_{i=1} P(x_i) \mbox{log}_2 P(x_i)$%; P(x;) (10.21)
\item $H(Y) = - \sum ^{n}_{j=1} P(y_j) \mbox{log}_2 P(y_j)$
\item $H(X, Y)= - \sum ^{n}_{j=1}\sum ^{m}_{i=1} P(x_i,y_j) \mbox{log}_2 P(x_i,y_j)$
\end{itemize}


%---------------------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information: Joint Entropy}
\vspace{-1cm}
These entropies can be interpreted as follows:\begin{itemize}\item $ H(X)$ is the average uncertainty of the channel input,
and $H(Y)$ is the average uncertainty of the channel output.\item  The joint entropy $H(X,Y)$ is the average uncertainty of the communication channel as a
whole.\end{itemize}

%---------------------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information: Conditional Entropy}
\begin{itemize}
\item The conditional entropy $H(X|Y)$ is a
measure of the average uncertainty remaining about the channel input after the channel output has
been observed. \item This is sometimes called the equivocation of X with respect to Y.  \item The
conditional entropy $H(Y|X)$ is the average uncertainty of the channel output given that X was
transmitted.
\end{itemize}
\begin{itemize}
\item $H(X|Y)= - \sum ^{n}_{j=1}\sum ^{m}_{i=1} P(x_i,y_j) \mbox{log}_2 P(x_i|y_j)$
\item $H(Y|X)= - \sum ^{n}_{j=1}\sum ^{m}_{i=1} P(x_i,y_j) \mbox{log}_2 P(y_j|x_i)$
\end{itemize}

%-----------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information : Useful Identities}
Two useful relationships among the types of entropies are
\begin{itemize}
\item $H(X,Y)=H(X|Y)+H(Y) $
\item $H(X,Y)=H(Y|X)+H(X) $
\end{itemize}
(Remark : compare to identities in probability theory)

%-----------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information : Useful Identities}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{./11BMutualInfo}
\caption{}
\label{fig:11BMutualInfo}
\end{figure}








\noindent \textbf{Mutual Information}
The mutual information $I(X; Y)$ of a channel is defined by
\[ I(X; Y) = H(X) -  H(X|Y) \mbox{    (b/symbol) } \]

Alternatively we can define it as either
\[ I(X; Y) = H(Y) -  H(Y|X) \mbox{     (b/symbol) } \]
 or as
\[ I(X; Y) = H(Y)+ H(Y)  - H(X,Y) \mbox{    (b/symbol) } \]
Remark: The
mutual information is the reduction of
entropy of X when Y
is
known.